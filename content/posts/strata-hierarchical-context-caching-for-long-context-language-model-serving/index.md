---
title: "[Paper Note] Strata Hierarchical Context Caching for Long Context Language Model Serving"
date: 2025-09-13
mdate: 2025-10-23T21:37:31-07:00
comment: true
weight: 0
author:
  name: "Jun"
  link: "https://github.com/kongjun18"
  avatar: "/images/avatar.jpg"
license: "All rights reserved"

categories:
- Paper

hiddenFromHomePage: false
hiddenFromSearch: false

summary: ""
resources:
- name: featured-image-preview
  src: 

toc:
  enable: true
math:
  enable: true
lightgallery: false
seo:
  images: []

repost:
  enable: true
  url: ""
---

---
created: 2025-09-13
---
	Status: #paper 
## layout
æˆ‘çš„é—®é¢˜æ˜¯ï¼Œåœ¨è¿™é‡Œæ‹·è´å®é™…ä¸Šæ˜¯128Bçš„å°æ®µï¼Œè€Œä¸æ˜¯è¿ç»­çš„æ•´ä¸ªpage
ChatGPT è¯´ï¼š

éå¸¸ç²¾å‡†çš„é—®é¢˜ ğŸ‘ï¼Œè¿™æ­£å¥½åˆ‡ä¸­ Strata è®¾è®¡é‡Œæœ€å®¹æ˜“è¢«è¯¯è§£çš„ä¸€å±‚ã€‚
ä½ å®Œå…¨å¯¹ï¼šæ¯ä¸ª GPU çº¿ç¨‹ç¡®å®åªæ‹· 128 B çš„å°æ®µï¼Œ
ä½†å…³é”®åœ¨äºâ€”â€”è¿™äº›ã€Œå°æ®µã€åœ¨ç‰©ç† I/O å±‚é¢ä¸Šä»ç„¶æ˜¯è¿ç»­çš„ DMA è®¿é—®ï¼Œå¹¶éçœŸæ­£æ„ä¹‰ä¸Šçš„éšæœºç¢ç‰‡ã€‚

è¿™çœ‹ä¼¼çŸ›ç›¾ï¼Œå…¶å®æ˜¯å› ä¸º GPU-assisted I/O çš„å·¥ä½œæ–¹å¼è·Ÿ CPU é©±åŠ¨çš„ memcpy å®Œå…¨ä¸åŒã€‚ä¸‹é¢æˆ‘åˆ†å±‚è§£é‡Šæ¸…æ¥šğŸ‘‡

ğŸ§© ä¸€ã€åˆ†æ¸…ä¸¤ä¸ªå±‚é¢
å±‚çº§	è°å‘èµ·	è®¿é—®æ¨¡å¼	å¸¦å®½ç“¶é¢ˆ
PCIe / NVLink å±‚ï¼ˆDMA å±‚ï¼‰	GPU ç¡¬ä»¶é€šé“	è¿ç»­è¯»å†™ (page çº§)	å†³å®š Sï¼ˆå•æ¬¡ä¼ è¾“å¤§å°ï¼‰
GPU å†…æ ¸å±‚ï¼ˆçº¿ç¨‹çº§è®¿å­˜ï¼‰	æ•°åƒä¸ªçº¿ç¨‹	å±€éƒ¨é¡ºåºè®¿é—® (128 B)	å†³å®šè®¿é—®å¹¶è¡Œåº¦ä¸åœ°å€è®¡ç®—

æ‰€ä»¥è™½ç„¶çº¿ç¨‹å•ä½æ˜¯ 128 Bï¼Œä½†è¿™äº›çº¿ç¨‹å®é™…ä¸Šåœ¨æ‰§è¡Œçš„æ˜¯ï¼š

ä»ä¸€ä¸ªè¿ç»­çš„ page åŒºé—´ä¸­ï¼Œæˆæ‰¹å– 128 B å­å—çš„å¹¶è¡Œ load/storeã€‚

è¿™æ„å‘³ç€ï¼š

PCIe æ€»çº¿ä¸Šä¼ è¾“çš„ä»ç„¶æ˜¯è¿ç»­çš„å¤§å—ï¼›

GPU åœ¨è¿™å—å¤§æ•°æ®ä¸Šåšçš„æ˜¯coalesced memory accessï¼ˆåˆå¹¶è®¿å­˜ï¼‰ï¼›

æ‰€ä»¥ DMA ä»ç„¶èƒ½è·‘æ»¡å¸¦å®½ï¼ŒS æ²¡æœ‰å˜å°ã€‚

âš™ï¸ äºŒã€ä¸ºä»€ä¹ˆ 128 B è®¿é—®ä¸ä¼šè®© S å˜å°

æˆ‘ä»¬å¯ä»¥ä» GPU å†…æ ¸å’Œ DMA çš„ååŒæœºåˆ¶ç†è§£è¿™ç‚¹ã€‚

1ï¸âƒ£ DMA ä¸å…³å¿ƒã€Œçº¿ç¨‹ç²’åº¦ã€

PCIe / NVLink çš„åº•å±‚ DMA å•å…ƒçœ‹è§çš„æ˜¯ GPU åœ¨å‘å‡ºå¤§é‡è¿ç»­åœ°å€çš„ load/store è¯·æ±‚ã€‚
åªè¦è¿™äº›åœ°å€è½åœ¨åŒä¸€è¿ç»­ç‰©ç†æ®µï¼ˆå³ä¸€ä¸ª pageï¼‰ï¼ŒDMA ä¼šè‡ªåŠ¨åˆå¹¶ä¸ºä¸€ä¸ªé¡ºåº burstã€‚

æ¢å¥è¯è¯´ï¼š

å¤šä¸ªçº¿ç¨‹çš„ 128 B è®¿é—®ï¼Œåœ¨ç‰©ç†å±‚æ˜¯åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„ MB çº§ burst I/Oã€‚

è¿™æ˜¯å› ä¸ºï¼š

128 B æ˜¯ GPU warp å†…ç†æƒ³çš„ coalesced access ç²’åº¦ï¼›

ä¸€ä¸ª warpï¼ˆ32 ä¸ªçº¿ç¨‹ï¼‰ä¸€æ¬¡èƒ½è®¿é—® 4 KBï¼›

å‡ åƒä¸ªçº¿ç¨‹è¦†ç›–æ•´é¡µå‡  MB æ•°æ®ï¼›

è¿™äº›è®¿é—®åˆèµ·æ¥åœ¨ DMA çœ¼é‡Œå°±æ˜¯é¡ºåºè®¿é—®ã€‚

2ï¸âƒ£ Strata æå‰ä¿è¯ page-first çš„ã€Œé¡µå†…è¿ç»­æ€§ã€

åœ¨ host å†…å­˜å¸ƒå±€ä¸Šï¼Œpage-first layout æœ¬èº«ä¿è¯ï¼š

Page1 = [L0_T1, L1_T1, L2_T1, ... L63_T1]   // è¿ç»­
Page2 = [L0_T2, L1_T2, L2_T2, ... L63_T2]   // è¿ç»­


æ‰€ä»¥å½“ GPU çº¿ç¨‹ç¾¤ä» Page1 è¯»å–æ—¶ï¼Œå®ƒä»¬çš„åœ°å€åˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œåªæ˜¯çº¿ç¨‹å†…çš„é€»è¾‘ stride ä¸åŒï¼ˆæ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸åŒåç§»ï¼‰ã€‚
è¿™å°±è®©ç¡¬ä»¶ DMA é€šé“å¯ä»¥æ•´é¡µé¡ºåºæ¬è¿ï¼Œè€Œä¸ç”¨å‘å‡ºç¢ç‰‡åŒ–å‘½ä»¤ã€‚

å› æ­¤ï¼š

çº¿ç¨‹ 128 B â‰  DMA 128 Bã€‚
çº¿ç¨‹ç²’åº¦å°åªæ˜¯é€»è¾‘åˆ’åˆ†ï¼›DMA ä»ç„¶ä¼ è¾“è¿ç»­å¤§å—ã€‚

ğŸ§  ä¸‰ã€Strata å†…æ ¸çš„æ‰§è¡Œæ¨¡å‹

è®ºæ–‡ Â§4.2 æè¿°çš„ GPU-assisted I/O kernel æµç¨‹æ˜¯è¿™æ ·çš„

Xie et al. - 2025 - Strata Hierâ€¦

ï¼š

å¯åŠ¨å°‘é‡å¤§ CUDA blocksï¼ˆæ¯ block 1024 çº¿ç¨‹ï¼‰ï¼›

æ¯ä¸ªçº¿ç¨‹ä» page-first æºï¼ˆCPU pinned memï¼‰è¯»å…¥ 128 Bï¼›

æ ¹æ® layer-first çš„åç§»ç®—å‡ºç›®æ ‡å†™å…¥ä½ç½®ï¼›

å†™å…¥ GPU HBMã€‚

ç”±äºï¼š

æºåœ°å€åŒºåŸŸåœ¨ç‰©ç†ä¸Šæ˜¯è¿ç»­çš„ï¼›

warp å†…çº¿ç¨‹è®¿é—®é‚»è¿‘åœ°å€ï¼›

GPU çš„ L2 cache + DMA æ§åˆ¶å™¨ä¼šåˆå¹¶è¿™äº›å°è¯·æ±‚ï¼›
å› æ­¤æ•´ä¸ª kernel å®é™…ä¸Šè¡¨ç°ä¸ºä¸€æ¬¡è¿ç»­ I/Oã€‚

è®ºæ–‡ä¹Ÿè¯´æ˜ï¼š

â€œThe granularity required for efficient GPU-assisted I/O is only 128 bytes ... sufficiently fine for single-page KV caches (kilobytes), eliminating the need to inflate page size for efficiency.â€
â€”â€”å³å³ä½¿çº¿ç¨‹æ¬ 128 Bï¼Œæ•´ä½“ I/O ä»ç„¶èƒ½æ»¡å¸¦å®½ï¼Œä¸å¿…å¢å¤§é¡µã€‚

è¿™è¯´æ˜ï¼š

Strata çš„å†…æ ¸è®¿é—®æ¨¡å¼ç»è¿‡ä¸“é—¨ä¼˜åŒ–ï¼Œè®© 128 B çº¿ç¨‹ç²’åº¦ä»èƒ½è§¦å‘é«˜æ•ˆé¡ºåº DMAã€‚

ğŸ“ˆ å››ã€S åœ¨å“ªé‡Œè¢«æ”¾å¤§

ä»å…¬å¼ 
ğ‘‹
=
ğ¶
â‹…
ğ‘†
/
ğ¿
X=Câ‹…S/L æ¥çœ‹ï¼š

å˜é‡	å¯¹åº”æœºåˆ¶	å½±å“
Cï¼ˆå¹¶å‘åº¦ï¼‰	GPU ä¸Šå‡ åƒçº¿ç¨‹å¹¶è¡Œè¯»å†™	æé«˜
Sï¼ˆå•æ¬¡ä¼ è¾“å¤§å°ï¼‰	Page-first layout ä¿è¯é¡µå†…è¿ç»­ï¼ŒDMA æŒ‰é¡µ burst	æå‡è‡³ MB çº§
Lï¼ˆå»¶è¿Ÿï¼‰	GPU kernel é¿å… CPU è°ƒåº¦å¼€é”€	é™ä½

æ‰€ä»¥ page-first layout ä¸æ˜¯è®©çº¿ç¨‹æ‹·æ›´å¤§å—ï¼Œè€Œæ˜¯è®© DMA èƒ½çœ‹åˆ°æ›´å¤§çš„è¿ç»­å—ã€‚
è€Œ GPU çº¿ç¨‹åªæ˜¯â€œç”¨å°ç²’åº¦å¹¶è¡Œåœ°å¡«æ»¡è¿™ä¸ªå¤§å—â€ã€‚

âœ… äº”ã€æ€»ç»“ä¸€å¥è¯

Strata çš„æ¯ä¸ªçº¿ç¨‹æ‹· 128 Bï¼Œä½†è¿™äº›çº¿ç¨‹ååŒè®¿é—®åŒä¸€ä¸ªè¿ç»­çš„ pageã€‚
è¿™æ · DMA å±‚çœ‹åˆ°çš„ä»æ˜¯å¤§å—é¡ºåºè®¿é—®ï¼ˆSâ†‘ï¼‰ï¼Œè€Œä¸æ˜¯ç¢ç‰‡åŒ– I/Oã€‚
page-first layout çš„ä½œç”¨ï¼Œå°±æ˜¯è®©è¿™ç§çº¿ç¨‹çº§ coalesced è®¿é—®æˆä¸ºå¯èƒ½ï¼Œä»è€Œæ—¢ä¿æŒå°é¡µç¼“å­˜ç²’åº¦ï¼Œåˆæ¢å¤å¤§å—å¸¦å®½ã€‚

---

Existing works assume the GPU-CPU bandwidth is not a bottleneck. However, this paper finds the GPU-CPU bandwidth is extremely under-utilized due to the data fragmentation introduced by *PagedAttention* and the existing scheduler doesn't treat GPU-CPU bandwidth as the first class resource, leading to severe data loading latency and thereby stalling the CPU. 

Experiments show 70% time spends in KV cache loading from the lower storage hierarchy, demonstrating GPU-CPU bandwidth is under-utilized. Even with an I/O mechanism achieving 75% of theoretical PCIe bandwidth, stalls still account for up to 24% of prefill execution time, calling for more sophisticated scheduling strategies.

The solution is as follows:
- Improve IO throughput
	- GPU-assisted IO increases concurrency: Thousands of GPU threads load data concurrently.
	- Page-first Layout increases unit data size: Manage the token's pages of layers into a contiguous layout in the low-hierarchical storage, such as GPU memory and SSD.
- IO-aware scheduling that treats GPU-CPU bandwidth as the first class resources.
	- Identify and solve the [delayed hit problem](https://dl.acm.org/doi/pdf/10.1145/3387514.3405883): Avoid scheduling multiple requests which encounter the same cache miss into a batch.
	- Balanced batch: Mix compute-bound requests with IO-bound requests (e.g., requests require loading data from the CPU memory) to maximize IO overlapping.
	- Hide loading stall with bubble filling: If an IO-bound request's latency cannot be hidden, schedule a compute-intensive requests to fill the pipeline bubble.

The evaluation shows:
- The existing designs cannot fully utilize the theoretical GPU-CPU bandwidth.
- With the aforementioned techniques, H200 machine can achieve high GPU-CPU bandwidth than [GH200 Grace Hopper](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/) with higher theoretical bandwidth.
- Most performance improvement is from GPU-assisted IO and solved delayed hit problem.

Insights:
- Long context is the future direction and recompute is not feasible in long context situations due to limited GPU HBM capacity, necessitating multi-tier KV cache offloading.
- KV cache loading from SSD should be interrupted if the loading doesn't finish before the request is scheduled.
- IO overlaying makes sense because computing and loading presents different IO patterns, one is HBM bandwidth bound and the other one is GPU-CPU bandwidth bound.

Thoughts:
- This paper only solves the GPU-CPU bottleneck and doesn't solve the SSD bottleneck.
- To some senses, this work centers the multi-tier KV cache storage around the CPU memory. 



---
## References
